{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "from tqdm.auto import tqdm\n",
    "from random import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8] # Magic square for 3x3 Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeState:\n",
    "    '''\n",
    "    Represents the state of a Tic-Tac-Toe game.\n",
    "    \n",
    "    Attributes:\n",
    "    - x (list): The list of cells occupied by 'x'.\n",
    "    - o (list): The list of cells occupied by 'o'.\n",
    "    \n",
    "    Methods:\n",
    "    - get_board: Returns the current state of the board as a 3x3 numpy array.\n",
    "    - get_actions: Returns the list of possible actions from the current state.\n",
    "    - step: Returns the next state and reward after taking an action.\n",
    "    - get_reward: Returns the reward for the current state.\n",
    "    - is_terminal: Returns whether the current state is terminal.\n",
    "    '''\n",
    "    def __init__(self, x, o) -> None:\n",
    "        self.x = x\n",
    "        self.o = o\n",
    "    \n",
    "    def get_board(self):\n",
    "        \"\"\"\n",
    "        Returns the current state of the board as a 3x3 numpy array.\n",
    "\n",
    "        The board is represented by a list of length 9, where each element represents a cell on the board.\n",
    "        The value 0 represents an empty cell, 1 represents a cell occupied by 'x', and 2 represents a cell occupied by 'o'.\n",
    "\n",
    "        Returns:\n",
    "        np.ndarray: The current state of the board as a 3x3 numpy array.\n",
    "        \"\"\"\n",
    "        board = [0 for _ in range(9)]\n",
    "        for e in self.x:\n",
    "            board[MAGIC.index(e)] = 1\n",
    "        for e in self.o:\n",
    "            board[MAGIC.index(e)] = 2\n",
    "        \n",
    "        return board\n",
    "\n",
    "    def available_actions(self):\n",
    "        \"\"\"\n",
    "        Returns the list of possible actions from the current state.\n",
    "\n",
    "        The list of possible actions is represented by a list of length 9, where each element represents a cell on the board.\n",
    "        The value 0 represents an empty cell, 1 represents a cell occupied by 'x', and 2 represents a cell occupied by 'o'.\n",
    "\n",
    "        Returns:\n",
    "        list: The list of possible actions from the current state.\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for i in range(9):\n",
    "            if MAGIC[i] not in self.x and MAGIC[i] not in self.o:\n",
    "                actions.append(i)\n",
    "        return actions\n",
    "    \n",
    "    def not_available_actions(self):\n",
    "        \"\"\"\n",
    "        Returns the list of impossible actions from the current state.\n",
    "\n",
    "        The list of impossible actions is represented by a list of length 9, where each element represents a cell on the board.\n",
    "        The value 0 represents an empty cell, 1 represents a cell occupied by 'x', and 2 represents a cell occupied by 'o'.\n",
    "\n",
    "        Returns:\n",
    "        list: The list of impossible actions from the current state.\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for i in range(9):\n",
    "            if MAGIC[i] in self.x or MAGIC[i] in self.o:\n",
    "                actions.append(i)\n",
    "        return actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns the next state and reward after taking an action.\n",
    "\n",
    "        Args:\n",
    "        action (int): The action to take.\n",
    "\n",
    "        Returns:\n",
    "        (TicTacToeState, int): The next state and reward after taking an action.\n",
    "        \"\"\"\n",
    "        if action not in self.available_actions():\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "        \n",
    "        x = copy(self.x)\n",
    "        o = copy(self.o)\n",
    "        if len(x) == len(o):\n",
    "            x.append(MAGIC[action])\n",
    "        else:\n",
    "            o.append(MAGIC[action])\n",
    "        \n",
    "        return TicTacToeState(x, o), self.get_reward()\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        Returns the reward for the current state.\n",
    "\n",
    "        Returns:\n",
    "        int: The reward for the current state.\n",
    "        \"\"\"\n",
    "        if self.is_terminal():\n",
    "            if len(self.x) > len(self.o):\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"\n",
    "        Returns whether the current state is terminal.\n",
    "\n",
    "        Returns:\n",
    "        bool: Whether the current state is terminal.\n",
    "        \"\"\"\n",
    "        if len(self.x) + len(self.o) == 9:\n",
    "            return True\n",
    "        \n",
    "        for c in combinations(self.x, 3):\n",
    "            if sum(c) == 15:\n",
    "                return True\n",
    "        for c in combinations(self.o, 3):\n",
    "            if sum(c) == 15:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def print_board(self):\n",
    "        \"\"\"\n",
    "        Prints the current state of the board.\n",
    "        \"\"\"\n",
    "        board = np.array(self.get_board()).reshape(3, 3)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if board[i][j] == 0:\n",
    "                    print('.', end='')\n",
    "                elif board[i][j] == 1:\n",
    "                    print('X', end='')\n",
    "                else:\n",
    "                    print('O', end='')\n",
    "            print()\n",
    "    \n",
    "    \n",
    "class TicTacToeAgent:\n",
    "    '''\n",
    "    Represents a Tic-Tac-Toe agent.\n",
    "    \n",
    "    Attributes:\n",
    "    - alpha (float): The learning rate.\n",
    "    - gamma (float): The discount factor.\n",
    "    - epsilon (float): The probability of taking a random action.\n",
    "    \n",
    "    Methods:\n",
    "    - take_action: Returns the action to take given the current state.\n",
    "    - train: Updates the Q table given the current state, action, next state, and reward.\n",
    "    '''\n",
    "    def __init__(self, alpha=0.1, gamma=0.5, epsilon=0.1) -> None:\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(9))\n",
    "           \n",
    "    def take_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns the action to take given the current state.\n",
    "\n",
    "        Args:\n",
    "        state (TicTacToeState): The current state.\n",
    "\n",
    "        Returns:\n",
    "        int: The action to take given the current state.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Q[tuple(state.get_board())][state.not_available_actions()] = -100\n",
    "        \n",
    "        pprint(self.Q[tuple(state.get_board())])\n",
    "        \n",
    "        if random() < self.epsilon:\n",
    "            return np.random.choice(state.available_actions())\n",
    "        else:\n",
    "            return np.argmax(self.Q[tuple(state.get_board())])\n",
    "    \n",
    "    def train(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        Updates the Q table given the current state, action, next state, and reward.\n",
    "\n",
    "        Args:\n",
    "        state (TicTacToeState): The current state.\n",
    "        action (int): The action taken.\n",
    "        next_state (TicTacToeState): The next state.\n",
    "        reward (int): The reward received.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Q[tuple(state.get_board())][action] += self.alpha * (reward + self.gamma * np.max(self.Q[tuple(state.get_board())]) - self.Q[tuple(state.get_board())][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ed2d8abdb449c59c4478aff72139a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "X..\n",
      "...\n",
      "...\n",
      "array([-100.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.])\n",
      "X.O\n",
      "...\n",
      "...\n",
      "array([-100.,    0., -100.,    0.,    0.,    0.,    0.,    0.,    0.])\n",
      "XXO\n",
      "...\n",
      "...\n",
      "array([-100., -100., -100.,    0.,    0.,    0.,    0.,    0.,    0.])\n",
      "XXO\n",
      "O..\n",
      "...\n",
      "array([-100., -100., -100., -100.,    0.,    0.,    0.,    0.,    0.])\n",
      "XXO\n",
      "OX.\n",
      "...\n",
      "array([-100., -100., -100., -100., -100.,    0.,    0.,    0.,    0.])\n",
      "XXO\n",
      "OXO\n",
      "...\n",
      "array([-100., -100., -100., -100., -100., -100.,    0.,    0.,    0.])\n",
      "XXO\n",
      "OXO\n",
      "X..\n",
      "array([-100., -100., -100., -100., -100., -100., -100.,    0.,    0.])\n",
      "XXO\n",
      "OXO\n",
      "XO.\n",
      "array([-100., -100., -100., -100., -100., -100., -100., -100.,    0.])\n",
      "XXO\n",
      "OXO\n",
      "XOX\n",
      "array([-100., -100., -100., -100., -100., -100., -100., -100., -100.])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid action.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m      8\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtake_action(state)\n\u001b[0;32m----> 9\u001b[0m next_state, reward \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain(state, action, next_state, reward)\n\u001b[1;32m     11\u001b[0m next_state\u001b[38;5;241m.\u001b[39mprint_board()\n",
      "Cell \u001b[0;32mIn[103], line 81\u001b[0m, in \u001b[0;36mTicTacToeState.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03mReturns the next state and reward after taking an action.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;124;03m(TicTacToeState, int): The next state and reward after taking an action.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable_actions():\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid action.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     83\u001b[0m x \u001b[38;5;241m=\u001b[39m copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m     84\u001b[0m o \u001b[38;5;241m=\u001b[39m copy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid action."
     ]
    }
   ],
   "source": [
    "agent = TicTacToeAgent()\n",
    "\n",
    "for _ in tqdm(range(10_000)):\n",
    "    state = TicTacToeState([], [])\n",
    "    while not state.is_terminal():\n",
    "        action = agent.take_action(state)\n",
    "        next_state, reward = state.step(action)\n",
    "        agent.train(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        state.print_board()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
