{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !! WORK IN PROGRESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "from tqdm.auto import tqdm\n",
    "from random import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8] # Magic square for 3x3 Tic-Tac-Toe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TicTacToeState:\n",
    "    '''\n",
    "    Represents the state of a Tic-Tac-Toe game.\n",
    "    \n",
    "    Attributes:\n",
    "    - x (list): The list of cells occupied by 'x'.\n",
    "    - o (list): The list of cells occupied by 'o'.\n",
    "    \n",
    "    Methods:\n",
    "    - get_board: Returns the current state of the board as a 3x3 numpy array.\n",
    "    - get_actions: Returns the list of possible actions from the current state.\n",
    "    - step: Returns the next state and reward after taking an action.\n",
    "    - get_reward: Returns the reward for the current state.\n",
    "    - is_terminal: Returns whether the current state is terminal.\n",
    "    - is_win: Returns whether the current state is a win for the given player.\n",
    "    - print_board: Prints the current state of the board.\n",
    "    '''\n",
    "    def __init__(self, x, o) -> None:\n",
    "        self.x = x\n",
    "        self.o = o\n",
    "    \n",
    "    def get_board(self):\n",
    "        \"\"\"\n",
    "        Returns the current state of the board as a 3x3 numpy array.\n",
    "\n",
    "        The board is represented by a list of length 9, where each element represents a cell on the board.\n",
    "        The value 0 represents an empty cell, 1 represents a cell occupied by 'x', and 2 represents a cell occupied by 'o'.\n",
    "\n",
    "        Returns:\n",
    "        - np.ndarray: The current state of the board as a 3x3 numpy array.\n",
    "        \"\"\"\n",
    "        board = [0 for _ in range(9)]\n",
    "        for e in self.x:\n",
    "            board[MAGIC.index(e)] = 1\n",
    "        for e in self.o:\n",
    "            board[MAGIC.index(e)] = 2\n",
    "        \n",
    "        return board\n",
    "\n",
    "    def available_actions(self):\n",
    "        \"\"\"\n",
    "        Returns the list of possible actions from the current state.\n",
    "\n",
    "        The list of possible actions is represented by a list of length 9, where each element represents a cell on the board.\n",
    "        The value 0 represents an empty cell, 1 represents a cell occupied by 'x', and 2 represents a cell occupied by 'o'.\n",
    "\n",
    "        Returns:\n",
    "        - list: The list of possible actions from the current state.\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for i in range(9):\n",
    "            if MAGIC[i] not in self.x and MAGIC[i] not in self.o:\n",
    "                actions.append(i)\n",
    "        return actions\n",
    "    \n",
    "    def not_available_actions(self):\n",
    "        \"\"\"\n",
    "        Returns the list of impossible actions from the current state.\n",
    "\n",
    "        The list of impossible actions is represented by a list of length 9, where each element represents a cell on the board.\n",
    "        The value 0 represents an empty cell, 1 represents a cell occupied by 'x', and 2 represents a cell occupied by 'o'.\n",
    "\n",
    "        Returns:\n",
    "        - list: The list of impossible actions from the current state.\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        for i in range(9):\n",
    "            if MAGIC[i] in self.x or MAGIC[i] in self.o:\n",
    "                actions.append(i)\n",
    "        return actions\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Returns the next state and reward after taking an action.\n",
    "\n",
    "        Args:\n",
    "        - action (int): The action to take.\n",
    "\n",
    "        Returns:\n",
    "        - (TicTacToeState, int): The next state and reward after taking an action.\n",
    "        \"\"\"\n",
    "        if action not in self.available_actions():\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "        \n",
    "        x = copy(self.x)\n",
    "        o = copy(self.o)\n",
    "        if len(x) == len(o):\n",
    "            x.append(MAGIC[action])\n",
    "        else:\n",
    "            o.append(MAGIC[action])\n",
    "            \n",
    "        next_state = TicTacToeState(x, o)\n",
    "        \n",
    "        return next_state, next_state.get_reward()\n",
    "    \n",
    "    def is_win(self, player):\n",
    "        \"\"\"\n",
    "        Returns whether the current state is a win for the given player.\n",
    "\n",
    "        Args:\n",
    "        - player (str): The player to check for a win.\n",
    "\n",
    "        Returns:\n",
    "        - bool: Whether the current state is a win for the given player.\n",
    "        \"\"\"\n",
    "        if player == 'x':\n",
    "            for c in combinations(self.x, 3):\n",
    "                if sum(c) == 15:\n",
    "                    return True\n",
    "        else:\n",
    "            for c in combinations(self.o, 3):\n",
    "                if sum(c) == 15:\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        Calculates the reward for the current state of the game.\n",
    "\n",
    "        Returns:\n",
    "        - float: The reward value.\n",
    "        \"\"\"\n",
    "        if self.is_terminal():\n",
    "            if len(self.x) > len(self.o):\n",
    "                if self.is_win('x'):\n",
    "                    return 100  # X wins\n",
    "                else:\n",
    "                    return 0  # Draw\n",
    "            elif self.is_win('o'):\n",
    "                return -100\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            reward = 0\n",
    "            for c in combinations(self.x, 2):\n",
    "                for a in set(MAGIC) ^ set(self.o):\n",
    "                    if sum(c) + a == 15:\n",
    "                        reward += 1\n",
    "\n",
    "            for c in combinations(self.x, 1):\n",
    "                for a in set(MAGIC) ^ set(self.o):\n",
    "                    if sum(c) + a == 15:\n",
    "                        reward += 0.5\n",
    "\n",
    "            return reward\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"\n",
    "        Returns whether the current state is terminal.\n",
    "\n",
    "        Returns:\n",
    "        - bool: Whether the current state is terminal.\n",
    "        \"\"\"\n",
    "        if len(self.x) + len(self.o) == 9:\n",
    "            return True\n",
    "        \n",
    "        for c in combinations(self.x, 3):\n",
    "            if sum(c) == 15:\n",
    "                return True\n",
    "        for c in combinations(self.o, 3):\n",
    "            if sum(c) == 15:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def print_board(self):\n",
    "        \"\"\"\n",
    "        Prints the current state of the board.\n",
    "        \"\"\"\n",
    "        board = np.array(self.get_board()).reshape(3, 3)\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                if board[i][j] == 0:\n",
    "                    print('.', end='')\n",
    "                elif board[i][j] == 1:\n",
    "                    print('X', end='')\n",
    "                else:\n",
    "                    print('O', end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToeAgent:\n",
    "    '''\n",
    "    Represents a Tic-Tac-Toe agent.\n",
    "    \n",
    "    Attributes:\n",
    "    - alpha (float): The learning rate.\n",
    "    - gamma (float): The discount factor.\n",
    "    - epsilon (float): The probability of taking a random action.\n",
    "    - Q (defaultdict): The Q table that stores the action-value estimates.\n",
    "    \n",
    "    Methods:\n",
    "    - take_action: Returns the action to take given the current state.\n",
    "    - train: Updates the Q table given the current state, action, next state, and reward.\n",
    "    '''\n",
    "    def __init__(self, alpha=0.1, gamma=0.5, epsilon=0.1) -> None:\n",
    "        '''\n",
    "        Initializes a TicTacToeAgent object.\n",
    "\n",
    "        Args:\n",
    "        - alpha (float): The learning rate. Default is 0.1.\n",
    "        - gamma (float): The discount factor. Default is 0.5.\n",
    "        - epsilon (float): The probability of taking a random action. Default is 0.1.\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = defaultdict(lambda: np.zeros(9))\n",
    "           \n",
    "    def take_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns the action to take given the current state.\n",
    "\n",
    "        Args:\n",
    "        - state (TicTacToeState): The current state.\n",
    "\n",
    "        Returns:\n",
    "        - int: The action to take given the current state.\n",
    "        \"\"\"\n",
    "        self.Q[tuple(state.get_board())][state.not_available_actions()] = -100\n",
    "        \n",
    "        if random() < self.epsilon:\n",
    "            return np.random.choice(state.available_actions())\n",
    "        else:\n",
    "            return np.argmax(self.Q[tuple(state.get_board())])\n",
    "    \n",
    "    def train(self, state, action, next_state, reward):\n",
    "        \"\"\"\n",
    "        Updates the Q table given the current state, action, next state, and reward.\n",
    "\n",
    "        Args:\n",
    "        - state (TicTacToeState): The current state.\n",
    "        - action (int): The action taken.\n",
    "        - next_state (TicTacToeState): The next state.\n",
    "        - reward (int): The reward received.\n",
    "        \"\"\"\n",
    "        self.Q[tuple(state.get_board())][action] += self.alpha * (reward + self.gamma * np.max(self.Q[tuple(state.get_board())]) - self.Q[tuple(state.get_board())][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code trains two TicTacToe agents using a reinforcement learning algorithm.\n",
    "The first agent, `agent`, has an epsilon value of 0.1, which determines the exploration-exploitation trade-off.\n",
    "The second agent, `agent_random`, has an epsilon value of 1, making it choose random actions.\n",
    "\n",
    "The training loop runs for 150,000 iterations. In each iteration:\n",
    "- A new TicTacToe state is created.\n",
    "- The first agent takes an action based on its current state and updates its internal model.\n",
    "- The next state and reward are obtained from the environment based on the chosen action.\n",
    "- The first agent trains on the observed transition by updating its internal model.\n",
    "- The state is updated to the next state.\n",
    "- If the state is terminal, the loop breaks.\n",
    "- The second agent takes an action based on the current state and updates its internal model.\n",
    "- The next state and reward are obtained from the environment based on the chosen action.\n",
    "- The first agent trains on the observed transition by updating its internal model.\n",
    "- The state is updated to the next state.\n",
    "- If the state is terminal, the loop breaks.\n",
    "\n",
    "Note: The commented out lines that call `state.print_board()` can be used to print the TicTacToe board at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34f9d512c0a463abd96c39a028ea964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = TicTacToeAgent(epsilon=0.1)\n",
    "agent_random = TicTacToeAgent(epsilon=1)\n",
    "\n",
    "for _ in tqdm(range(150_000)):\n",
    "    state = TicTacToeState([], [])\n",
    "    while not state.is_terminal():\n",
    "        action = agent.take_action(state)\n",
    "        next_state, reward = state.step(action)\n",
    "        agent.train(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        #state.print_board()\n",
    "\n",
    "        if state.is_terminal():\n",
    "            break\n",
    "        \n",
    "        action = agent_random.take_action(state)\n",
    "        next_state, reward = state.step(action)\n",
    "        agent.train(state, action, next_state, -reward)\n",
    "        state = next_state\n",
    "        #state.print_board()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_of_x_wins(n_games):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of games won by 'x' player out of the total number of games played.\n",
    "    \n",
    "    Parameters:\n",
    "    - n_games (int): The number of games to be played.\n",
    "    \n",
    "    Returns:\n",
    "    - float: The percentage of games won by 'x' player.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert n_games > 0\n",
    "    \n",
    "    x_wins = 0\n",
    "\n",
    "    for _ in tqdm(range(n_games)):\n",
    "        state = TicTacToeState([], [])\n",
    "        while not state.is_terminal():\n",
    "            action = agent.take_action(state)\n",
    "            next_state, _ = state.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            if state.is_terminal():\n",
    "                break\n",
    "\n",
    "            action = agent_random.take_action(state)\n",
    "            next_state, _ = state.step(action)\n",
    "            state = next_state\n",
    "\n",
    "        if state.is_win(\"x\"):\n",
    "            x_wins += 1\n",
    "\n",
    "    return x_wins / n_games * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af83c7e7b974091a6417d01633c4ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of games won by X: 90.84%\n"
     ]
    }
   ],
   "source": [
    "percentage_of_x_wins = calculate_percentage_of_x_wins(100_000)\n",
    "print(\"Percentage of games won by X: %.2f%%\" % percentage_of_x_wins)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
